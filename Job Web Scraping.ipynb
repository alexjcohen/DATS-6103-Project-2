{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = 'text-align: center;'>Analyzing Data Science Job Requirements</h1>\n",
    "<h3 style = 'text-align: center;'>Alex Cohen - DATS 6103</h3>\n",
    "<p> Applying to jobs can be stressful, especially when trying to balance the application process with school, existing work responsibilties, and general life requirements. This problem is compounded by the fact that 'data science' means so many different things to different people. </p>\n",
    "<p>Going into the application process with a better idea of what 'makes' a data scientist and exactly what companies are looking for when recruiting applicants can help make this process a little easier. For this, we will use <a href = 'https://www.indeed.com'>indeed.com</a>, a popular job search engine, to better understand what companies are looking for when they recruit data scientists.</p>\n",
    "\n",
    "<p>This will be done in two steps: </p>\n",
    "<ol> \n",
    "    <li>Use BeautifulSoup to gather job descriptions and company data</li> <p></p>\n",
    "    <li>Analyze the different requirements to see what these jobs have in common, and where they differ</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Use BeautifulSoup to get job descriptions and company data: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set a timer to determine run time\n",
    "start = datetime.now()\n",
    "run_date = datetime.date(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here is our dictionary of terms that we want to look for in job descriptions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of terms and then their associated regex values\n",
    "words = {    \n",
    "        # python and related terms\n",
    "        'python' : 'python',\n",
    "        'jupyter' : 'jupyter',\n",
    "        'pandas' : 'pandas',\n",
    "        'numpy' : 'numpy', \n",
    "        'tensorflow' : 'tensorflow', \n",
    "        'keras' : 'keras',\n",
    "        'sklearn' : 'sklearn',\n",
    "        \n",
    "        # r and related terms\n",
    "        'r' : 'r',\n",
    "        'rstudio' : 'rstudio',\n",
    "        'tidyverse': '(.?plyr|tidyverse)',\n",
    "        \n",
    "        # other programming languages\n",
    "        'java' : 'java',\n",
    "        'c' : 'c',\n",
    "        'c++' : 'c\\+{2}',\n",
    "        'c#' : 'c\\#',\n",
    "        'fortran' : 'fortran',\n",
    "    \n",
    "        # statistical/programming languages\n",
    "        'sas' : 'sas', \n",
    "        'matlab' : 'matlab',\n",
    "        'spss' : 'spss',\n",
    "        'stata' : 'stata',\n",
    "        'excel' : '(excel|vba)',\n",
    "              \n",
    "        # big data technology \n",
    "        'hadoop' : '(hadoop|hdfs)',\n",
    "        'spark' : 'spark',\n",
    "        'hive' : 'hive',\n",
    "        'pig' : 'pig',\n",
    "        'scala' : 'scala',\n",
    "\n",
    "        # data visualization\n",
    "        'tableau' : 'tableau',\n",
    "        'd3' : 'd3',\n",
    "        'bi' : '(bi|powerbi)',\n",
    "    \n",
    "        # database technology\n",
    "        'sql' : 'sql',\n",
    "        'nosql' : '(nosql|no sql)',\n",
    "        'mysql' : '(mysql|my sql)',\n",
    "        'postgresql' : 'postgresql',\n",
    "        'sql server': 'sql server',\n",
    "        'mongo' : '(mongo|mongodb)',\n",
    "        'redis' : 'redis',\n",
    "        'cassandra' : 'cassandra',\n",
    "        'neo4j' : '(neo4j|cypher)',\n",
    "    \n",
    "        # web-development related\n",
    "        'html' : 'html',\n",
    "        'css' : 'css',\n",
    "        'javascript' : 'javascript', \n",
    "        'ruby' : 'ruby',\n",
    "        'php' : 'php',\n",
    "        \n",
    "        # buzz-words\n",
    "        'api': 'api',\n",
    "        'aws': '(aws|amazon web services)', \n",
    "        'cloud':'cloud', \n",
    "        'big data' : 'big data', \n",
    "        'machine learning': '(machine learning|ml)', \n",
    "        'deep learning': '(deep learning|dl)', \n",
    "        'ai' : '(artificial intelligence|ai)',\n",
    "        'nlp': '(nlp|natural language processing)',\n",
    "        'graph' : '(graph|graph database)',\n",
    "        'data visual': 'data visual[a-z]+',\n",
    "        'predictive': 'predictive',\n",
    "        'model':'(model|modeling)',\n",
    "        'data mining':'data min[a-z]+',\n",
    "        'simulation': 'simulat[a-z]+',\n",
    "        'forecast' : '(forecast|forecasting)',\n",
    "    \n",
    "        # degree-related\n",
    "        'statistic': '(statistic|statistics)',\n",
    "        'math':'(math|mathematics)',\n",
    "        'computer science' : 'computer science',\n",
    "        'economics' : 'economics',\n",
    "        'engineering': 'engineering',\n",
    "    \n",
    "        # dc-specific\n",
    "        'clearance': 'clearance'\n",
    "    }\n",
    "\n",
    "# convert the values to regex values for searching the text - adding either a ' ', ',' or '/'\n",
    "# at either end to ensure we are truly capturing the value we want\n",
    "\n",
    "words.update((k,'(\\s|,|\\/|\\.)' + v + '(\\s|,|\\/|\\.)') for k,v in words.items())\n",
    "\n",
    "columns = list(words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style = 'text-align: center;'>Get individual job links</h4>\n",
    "<p>The first step will be to get a list of links to the individual job postings, since Indeed only returns a list of brief descriptions when you search for the term 'data scientist'.</p>\n",
    "<p>Additionally, we will not limit ourselves geographically, so our search area will be the 'United States', as you can see in the URL.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Job URLs\n",
      "\n",
      "..................................................\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "# define the number of jobs we want to parse (actual was 2 runs of 2500 jobs)\n",
    "nJobs = 2500\n",
    "links = []\n",
    "\n",
    "print('Getting Job URLs')\n",
    "print()\n",
    "\n",
    "# loop through Indeed.com (which shows 50 results per page) until we have individual job links\n",
    "# for all nJobs\n",
    "for i in range(0, nJobs, 50):\n",
    "    url = 'https://www.indeed.com/jobs?q=data+scientist&l=United+States&limit=50&start=' + str(i)\n",
    "    search = requests.get(url)\n",
    "    \n",
    "    # parse html for tags with related link class and append job urlsto running list\n",
    "    soup = BeautifulSoup(search.text, 'html.parser')\n",
    "    for link in soup.find_all('div', attrs = {'class':'jobsearch-SerpJobCard'}):\n",
    "        link = link.find('a')\n",
    "        links.append(link.get('href'))\n",
    "    print('.', end = '')\n",
    "\n",
    "print()\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links retrieved: 2504\n",
      "Example link: https://www.indeed.com/company/Numero-Data-LLC/jobs/Entry-Level-Data-Scientist-ad9a154fb51286b7?fccid=e2b02ba1480369a4&vjs=3\n"
     ]
    }
   ],
   "source": [
    "# append the first part of the URL to each link and remove paid ad jobs\n",
    "links = ['https://www.indeed.com' + s for s in links]\n",
    "links = [x for x in links if x.count('pagead') == 0]\n",
    "\n",
    "# show number of links and an example link (to prove it's the format we wanted)\n",
    "print('Number of links retrieved: ' + str(len(links)))\n",
    "print('Example link: ' + links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style = 'text-align: center;'> Parse Job Descriptions </h4>\n",
    "<p>Now that we have a list containing our individual job pages, we want to parse each of them and count the number of times each of our terms appear in the description.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Job Descriptions\n",
      "\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 50\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 100\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 150\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 200\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 250\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 300\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 350\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 400\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 450\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 500\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 550\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 600\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 650\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 700\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 750\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 800\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 850\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 900\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 950\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1000\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1050\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1100\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1150\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1200\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1250\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1300\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1350\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1400\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1450\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1500\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1550\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1600\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1650\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1700\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1750\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1800\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1850\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1900\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1950\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2000\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2050\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2100\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2150\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2200\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||| 2250\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||| 2300\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2350\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||| 2400\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2450\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||| 2500\n",
      "||||\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "# create emtpy data frame and counter\n",
    "output = pd.DataFrame(columns=words.keys())\n",
    "counter = 0\n",
    "\n",
    "print('Parsing Job Descriptions')\n",
    "print()\n",
    "\n",
    "# loop through each link\n",
    "for job in links:\n",
    "    \n",
    "    try: \n",
    "        counter += 1\n",
    "\n",
    "        # get the html content\n",
    "        page = requests.get(job)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # find the company link and company page\n",
    "        company = soup.find(class_ = 'icl-u-lg-mr--sm icl-u-xs-mr--xs')\n",
    "        companyPage = soup.find(class_ = \"icl-NavigableContainer-linkWrapper\")\n",
    "\n",
    "        # try to get the company page if there is a link\n",
    "        try:\n",
    "            companyPage = companyPage.get('href')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # try to get the company name if it exists\n",
    "        try:\n",
    "            company = company.text\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # find the job title\n",
    "        jobTitle = soup.find(class_ = 'jobsearch-JobInfoHeader-title')\n",
    "\n",
    "        try:\n",
    "            jobTitle = jobTitle.text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # get the location of the job\n",
    "        try:\n",
    "            location = soup.find(class_ = 'jobsearch-InlineCompanyRating')\n",
    "            location = location.find_all('div')[-1].text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # get the text of the job description and make it all lower case (to avoid miscouning\n",
    "        # due to capitalization differences)\n",
    "        temp = soup.find(class_ = 'jobsearch-jobDescriptionText')\n",
    "        temp = temp.get_text(\" \").lower()\n",
    "\n",
    "        wordMatch = []\n",
    "\n",
    "        # append the number of times each word in our dictionary appears in the description\n",
    "        for i in words.keys():\n",
    "            n = len(re.findall(words.get(i), temp))\n",
    "            data = [i, int(n)]\n",
    "            wordMatch.append(data)\n",
    "\n",
    "        # append on the job, company, title, and company page\n",
    "        data = ['job', job]\n",
    "        employer = ['company', company]\n",
    "        location = ['location', location]\n",
    "        title = ['job title', jobTitle]\n",
    "        employerPage = ['employer page', companyPage]\n",
    "\n",
    "        wordMatch.append(data)\n",
    "        wordMatch.append(employer)\n",
    "        wordMatch.append(location)\n",
    "        wordMatch.append(title)\n",
    "        wordMatch.append(employerPage)\n",
    "\n",
    "        # combine each job into a data frame and append\n",
    "        wordMatch = pd.DataFrame(wordMatch)\n",
    "        wordMatch = wordMatch.set_index([0]).T\n",
    "        output = output.append(wordMatch, sort = False)\n",
    "\n",
    "        # define the counter to measure progress\n",
    "        if counter % 50 == 0:\n",
    "            p = '| ' + str(int(50 * (counter / 50)))\n",
    "            end = '\\n'\n",
    "        else:\n",
    "            p = '|'\n",
    "            end = ''\n",
    "\n",
    "        print(p, end = end)\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print()\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style = 'text-align: center;'>Company Information</h4>\n",
    "<p>Now that we have the job descriptions and links to the employer pages, let's see if we can get information related to the size of the company (in both employees and revenue), and the company industry</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# attempt to add on the company page by splitting the employer link by the 'reviews' piece of the employer link\n",
    "comp = []\n",
    "for i in output['employer page']:\n",
    "    try:\n",
    "        temp = i.split('reviews?')[0]\n",
    "        comp.append(temp)\n",
    "    except:\n",
    "        comp.append(np.NaN)\n",
    "\n",
    "# create a data frame out of the job and company link\n",
    "comp = pd.DataFrame(list(zip(output['job'], comp)), columns = ['job', 'employer page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Company Information\n",
      "\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 50\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 100\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 150\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 200\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 250\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 300\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 350\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 400\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 450\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 500\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 550\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 600\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 650\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 700\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 750\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 800\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 850\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 900\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 950\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1000\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1050\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1100\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1150\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1200\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1250\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1300\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1350\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1400\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1450\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1500\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1550\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1600\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1650\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1700\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1750\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1800\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1850\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1900\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 1950\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2000\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2050\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2100\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2150\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2200\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2250\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2300\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2350\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2400\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2450\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||| 2500\n",
      "\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "# initialize the data frame\n",
    "compInfo = pd.DataFrame(columns = ['job', 'Link','Employees size', 'Industry', 'Revenue'])\n",
    "counter = 0\n",
    "\n",
    "print('Getting Company Information')\n",
    "print()\n",
    "\n",
    "# loop through companies\n",
    "for i, row in comp.iterrows():\n",
    "    counter += 1\n",
    "    url = row['employer page']\n",
    "    titles = []\n",
    "    values = []\n",
    "    temp = []\n",
    "    \n",
    "    # try to get the company page information, which include the Number of Employees, \n",
    "    # Industry, and Revenue\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        title = soup.find_all(class_ = 'cmp-AboutMetadata-itemTitle')\n",
    "        value = soup.find_all(class_ = 'cmp-AboutMetadata-itemCotent')\n",
    "        \n",
    "        # loop through the different itemcontent values and try to append (as there \n",
    "        # aren't defined html tags for each company item)\n",
    "        for t in title:\n",
    "            titles.append(t.text)\n",
    "        for v in value:\n",
    "            values.append(v.get_text('|').split('|')[0])\n",
    "        \n",
    "        temp = dict(zip(titles, values))\n",
    "\n",
    "    # if the page doesn't exist, set the different values to NaNs that we can filter out later\n",
    "    except:\n",
    "        temp = {'Employees Size': np.NaN,\n",
    "                'Industry': np.NaN,\n",
    "                'Revenue': np.NaN}\n",
    "    \n",
    "    # create a data frame with values and then append to company info\n",
    "    a = pd.DataFrame([row['job'], row['employer page'], temp.get('Employees size'), temp.get('Industry'), temp.get('Revenue')]).T\n",
    "    a.columns = compInfo.columns\n",
    "    compInfo = compInfo.append(a, ignore_index = True)\n",
    "    \n",
    "    # define our counter to measure progress\n",
    "    if counter % 50 == 0:\n",
    "        p = '| ' + str(int(50 * (counter / 50)))\n",
    "        end = '\\n'\n",
    "    else:\n",
    "        p = '|'\n",
    "        end = ''\n",
    "        \n",
    "    print(p, end = end)\n",
    "    \n",
    "print()\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# join job posting info with company info\n",
    "output = output.merge(compInfo, how = 'inner', on = ['job']).drop(columns = ['employer page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style = 'text-align: center;'>Bring it all together</h4>\n",
    "<p>Now that we have our company and job info, let's join the two data frames, split the job location to give us a city and state we can examine, and then write it all out to csv files (we could write this to a sql database if we had more data we wanted to store, or if we were periodically running this across weeks/months)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split location into city and state and append to end\n",
    "state = output['location'].str.replace('\\d+', '').str.split(',', expand = True).loc[:,1].str.strip()\n",
    "city = output['location'].str.replace('\\d+', '').str.split(',', expand = True).loc[:,0].str.strip()\n",
    "output['city'] = city\n",
    "output['state'] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write output as a csv file for further analysis (including the date and njobs to \n",
    "# provide more informative file names)\n",
    "output.drop_duplicates(keep = 'first', inplace = True)\n",
    "output.to_csv('output-' + str(nJobs) + '-' + str(run_date) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1289, 73)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get an idea of how many jobs we have after removing duplicate values\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0:30:24.193888\n"
     ]
    }
   ],
   "source": [
    "# find out how long it took to run\n",
    "print('Run time:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-weight: bold; text-align: center;'>Now that we have our data, let's head over to the job analysis.ipynb file to analyze our results</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
